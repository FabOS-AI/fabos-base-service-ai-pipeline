(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{294:function(e,t,a){"use strict";a.r(t);var n=a(15),l=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"evaluation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluation"}},[e._v("#")]),e._v(" Evaluation")]),e._v(" "),t("p",[e._v("This module is used for evaluation and benchmarking the AI models.\nIt runs on the port '5002'.")]),e._v(" "),t("h2",{attrs:{id:"apis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#apis"}},[e._v("#")]),e._v(" APIs")]),e._v(" "),t("details",{staticClass:"custom-block details"},[t("summary",[e._v("Evaluate")]),e._v(" "),t("p",[e._v("This api evaluates the given model in the http request and returns the evaluation results.\nIt is called by sending a request to the evaluation port + '/evaluate'")]),e._v(" "),t("p",[e._v("function:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("evaluate_model()\n\nThis function gets the testing dataset from the http request along with the zipped model to be evaluated.\nIt extracts the zipped model and adds it to temporary zipped model.::\n    \n    # Read bytes of model.zip and build a new zip file\n    filename = request.files['model'].filename\n    zipped_model = request.files['model'].read()\n    with open(filename + \".zip\", 'wb') as s:\n        s.write(zipped_model)\n\nIt calls the :ref:`eval.evaluate(filename, X_test, y_test, metric) <evalbackend>` for evaluation of the model.\n\n:return: dictionary of evaluation results of the given model\n:rtype: dictionary\n")])])])]),e._v(" "),t("details",{staticClass:"custom-block details"},[t("summary",[e._v("Benchmark")]),e._v(" "),t("p",[e._v("This api is called when multiple models are needed to be evaluated and benchmarked for optimization.\nIt is called by sending a request to the evaluation port + '/benchmark'")]),e._v(" "),t("p",[e._v("function:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('benchmark_models()\n\nIt reads the metrics and the list of models to be evaluated from the http request.\nCopies each model in the model list to the temp location so that the backend function can use them from that location for evaluation::\n\n    model_list = []\n    for file in request.files:\n\n        if "model" in file:\n            filename = request.files[file].filename\n            zipped_model = request.files[file].read()\n            with open(filename + ".zip", \'wb\') as s:\n                s.write(zipped_model)\n\n            model_list.append(filename)\n        else:\n            print("nope")\n\nThen it calls :ref:`eval.evaluate(filename, X_test, y_test, metric) <evalbackend>` for each model in the model list::\n    \n    results = {}\n    for model in model_list:\n        #Evaluate model and return loss\n        result = eval.evaluate(model, X_test, y_test, metric)\n        results[model] = result\n\n:return: It returns a json made from the dictionary of evaluation results of all the models\n:rtype: JSON\n')])])])])])}),[],!1,null,null,null);t.default=l.exports}}]);